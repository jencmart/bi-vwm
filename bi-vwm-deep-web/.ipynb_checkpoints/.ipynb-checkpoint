{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import string\n",
    "import re\n",
    "from pprint import pprint\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import math\n",
    "from time import sleep\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some shitty parsing helpers funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_leading_tailing_spaces_commas(s):\n",
    "    s1 = re.sub(r\"^\\s+|\\s+$\", \"\", s)\n",
    "    s2 = re.sub(r\"^\\,+|\\,+$\", \"\", s1)\n",
    "    s3 = re.sub(r\"^\\s+|\\s+$\", \"\", s2)\n",
    "    return s3\n",
    "\n",
    "\n",
    "def extract_first_number_or_zero(dirty):\n",
    "    dirty = re.sub(r\"\\D+\", \" \", dirty)\n",
    "    lst = [int(s) for s in dirty.split() if s.isdigit()]\n",
    "    if len(lst) > 0:\n",
    "        return lst[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def extract_last_number_or_zero(dirty):\n",
    "    dirty = re.sub(r\"\\D+\", \" \", dirty)\n",
    "    lst = [int(s) for s in dirty.split() if s.isdigit()]\n",
    "    if len(lst) > 0:\n",
    "        return lst[-1]\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def parse_price(buy): #  SAFE\n",
    "    if buy is None:\n",
    "        return 0, 0, 0\n",
    "    # -- p -- class: price\n",
    "    price = buy.find('p', attrs={'class':'price'})\n",
    "    if price is None:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    item_price_from = price.find('span', attrs={'class':'priceFrom'})\n",
    "    if item_price_from is None:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    item_price_from = extract_first_number_or_zero(item_price_from.getText())\n",
    "    price_to = price.find('span', attrs={'class':'priceTo'})\n",
    "    if price_to is None:\n",
    "        item_price_to = item_price_from\n",
    "        item_shop_count = 1\n",
    "    else:\n",
    "        item_price_to = price_to.getText()\n",
    "        item_price_to = extract_first_number_or_zero(item_price_to)\n",
    "        # -- p -- shop count (only if multiple prices)\n",
    "        item_shop_count = buy.find('p', attrs={'class':'count'})\n",
    "        if item_shop_count is None:\n",
    "            return item_price_from, item_price_to, 1\n",
    "        item_shop_count = item_shop_count.find('a')\n",
    "        if item_shop_count is None:\n",
    "            return item_price_from, item_price_to, 1\n",
    "        item_shop_count = extract_first_number_or_zero(item_shop_count.getText())\n",
    "\n",
    "    return item_price_from, item_price_to, item_shop_count\n",
    "\n",
    "\n",
    "def parse_reviews(desc): # SAFE\n",
    "     # -- p -- class: rw\n",
    "    reviews = desc.find('p', attrs={'class':'rw'})\n",
    "    if reviews is None :\n",
    "        return 0, 0\n",
    "    \n",
    "    item_rating = reviews.find('span', attrs={'class':'rating'})\n",
    "    if item_rating is None:\n",
    "        return 0, 0\n",
    "    item_rating = item_rating.find('span', attrs={'class':'hidden'})\n",
    "    if item_rating is None:\n",
    "        return 0, 0\n",
    "    item_rating = extract_first_number_or_zero(item_rating.getText())\n",
    "    \n",
    "    item_reviews_cnt = reviews.find('span', attrs={'class':'review-count'})\n",
    "    if item_reviews_cnt is None :\n",
    "        item_reviews_cnt = 0\n",
    "    else:\n",
    "        item_reviews_cnt = item_reviews_cnt.find('a')\n",
    "        if item_reviews_cnt is None:\n",
    "            return 0, 0\n",
    "        item_reviews_cnt = item_reviews_cnt.getText() \n",
    "        item_reviews_cnt = extract_first_number_or_zero(item_reviews_cnt)                \n",
    "        \n",
    "    return item_rating, item_reviews_cnt\n",
    "        \n",
    "def parse_description(desc): # SAFE\n",
    "    # -- div -- class: desc\n",
    "    desc_text = desc.find('p', attrs={'class':'desc'})\n",
    "    if desc_text is None:\n",
    "        return ''\n",
    "    \n",
    "    item_description = ''\n",
    "    for itm in desc_text:\n",
    "        if  type(itm) is bs4.element.NavigableString:\n",
    "            item_description = itm.string\n",
    "    return item_description\n",
    "\n",
    "\n",
    "def parse_id_and_name(desc): # SAFE\n",
    "    # -- div -- class: product-container\n",
    "    item_container = desc.find('div', attrs={'class':'product-container'})\n",
    "    if item_container is None:\n",
    "        return '', ''\n",
    "    item_id = item_container.get('id')\n",
    "    item_name = item_container.find('h2').find('a').getText()\n",
    "    \n",
    "    return item_id, item_name\n",
    "  \n",
    "    \n",
    "languages = ['anglické', 'chorvatské', 'dánské', 'francouzské', 'holandské', 'italské', 'japonské', 'latinské', 'maďarské', 'německé', 'polské', 'portugalské', 'ruské', 'slovenské', 'turecké', 'vietnamské', 'Česky', 'české', 'čínské', 'řecké', 'španělské']\n",
    "genres = ['beletrie', 'cestopisy', 'dobrodružství, humor', 'duchovno', 'dětské', 'esoterika', 'historická beletrie', 'horory', 'kuchařky', 'křížovky', 'naučná', 'náboženství', 'osobní rozvoj', 'poezie', 'populárně naučná', 'pro ženy', 'Psychologický román', 'román', 'romány pro ženy', 'sci-fi a fantasy', 'slovníky', 'zdraví a životní styl', 'životopisy']\n",
    "# last always language\n",
    "# between numbers - author\n",
    "# genres -- matchneme\n",
    "# problem: no numbers -- co je author a co je nakaladatelstvi\n",
    "# ['Grada', 'dětské', 'Antonín Šplíchal, české']\n",
    "# dobrodružství, Panteon, 2019, Jonas Jonasson, 400, české\n",
    "\n",
    "def parse_hard_shit(s):\n",
    "    gen = ''\n",
    "    lng = ''\n",
    "    prod = ''\n",
    "    auth = ''\n",
    "    s1 = re.sub(r'<p class=\"params\">', \"\", s)\n",
    "    s1 = re.sub(r'<abbr title=\"Počet stran\">[0-9]+</abbr>,', 'XXX', s1) # pages\n",
    "    s1 = re.sub(r'<abbr title=\"Rok vydání\">[0-9]+</abbr>,', 'XXX', s1) # year\n",
    "    result = [x.strip() for x in s1.split(',')]\n",
    "\n",
    "    for l in languages:\n",
    "        if l+'</p>' == result[-1]:\n",
    "            lng = l\n",
    "            s1 = re.sub(l+'</p>', 'XXX', s1)\n",
    "            break\n",
    "\n",
    "    result = [x.strip() for x in s1.split(',')]\n",
    "    for g in genres:\n",
    "        for r in result:\n",
    "            if g == r:\n",
    "                gen = g\n",
    "                s1 = re.sub(g+',', 'XXX', s1)\n",
    "                break\n",
    "\n",
    "    # chybi - autor && nakladatelstvi\n",
    "\n",
    "    s1 = re.sub(r'XXX', '', s1) # year\n",
    "    s1 = remove_leading_tailing_spaces_commas(s1)\n",
    "    result = [x.strip() for x in s1.split(',')]\n",
    "    result\n",
    "\n",
    "    if len(result) == 1:\n",
    "        auth = result[0]\n",
    "\n",
    "    if len(result) == 2:\n",
    "        prod = result[0]\n",
    "        auth = result[1]\n",
    "\n",
    "    if len(result) > 2:\n",
    "        for r in reversed(result):\n",
    "            if (' ' in r):\n",
    "                auth = r\n",
    "                break\n",
    "          \n",
    "    if auth != '':\n",
    "        result.remove(auth)\n",
    "        if prod == '':\n",
    "            if len(result) > 0:\n",
    "                for r in result:\n",
    "                    if r.isalpha():\n",
    "                        prod = r\n",
    "                        break\n",
    "    \n",
    "    prod = remove_leading_tailing_spaces_commas(prod)\n",
    "    auth = remove_leading_tailing_spaces_commas(auth)\n",
    "    return prod, gen, auth, lng\n",
    "\n",
    "def parse_params(desc): # SAFE\n",
    "    # -- p -- class: params\n",
    "    publisher = ''\n",
    "    genre = ''\n",
    "    author = ''\n",
    "    lang = ''\n",
    "    year = 0\n",
    "    pages_cnt = 0\n",
    "    \n",
    "    params = desc.find('p', attrs={'class':'params'})\n",
    "    if params is None:\n",
    "        return year, pages_cnt, publisher, genre, author, lang, \"\"\n",
    "    \n",
    "    unknown = []\n",
    "    for param in params:\n",
    "        if  type(param) is bs4.element.NavigableString:\n",
    "            unknown.append(remove_leading_tailing_spaces_commas(param.string))\n",
    "        elif param.get('title') == 'Rok vydání':\n",
    "            year = extract_first_number_or_zero(param.getText())\n",
    "        elif param.get('title') == 'Počet stran':\n",
    "            pages_cnt = extract_first_number_or_zero(param.getText())\n",
    "    params = str(params)\n",
    "    publisher, genre, author, lang = parse_hard_shit(params)\n",
    "    return year, pages_cnt, publisher, genre, author, lang, unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID, name, rating, reviews_cnt, year, pages_cnt, publisher, genre, author, lang, description, price_from, price_to, unparsed\n",
    "def parse_product(product):\n",
    "   \n",
    "    # [DESCRIPTION DIV]\n",
    "    desc = product.find('div', attrs={'class':'desc'})  # ok always\n",
    "    ID, name = parse_id_and_name(desc)  # SAFE\n",
    "    rating, reviews_cnt = parse_reviews(desc)  # SAFE\n",
    "    year, pages_cnt, publisher, genre, author, lang, unparsed = parse_params(desc)  # SAFE\n",
    "    description = parse_description(desc)  # SAFE\n",
    "    \n",
    "    # [BUY DIV]\n",
    "    buy = product.find('div', attrs={'class':'buy'})\n",
    "    price_from, price_to, shop_count = parse_price(buy)  # SAFE\n",
    "    \n",
    "    return [ID, name, rating, reviews_cnt, year, pages_cnt, publisher, genre, author, lang, description, price_from, price_to, unparsed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main crawl-loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count results helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnt_results_crawl(soup):  # SAFE\n",
    "    cnt_items = soup.find('div',  attrs={'class':'sort-bottom'})\n",
    "   \n",
    "    if cnt_items != None:\n",
    "        cnt_items = cnt_items.find('div',  attrs={'class':'count'})\n",
    "        if cnt_items != None:\n",
    "            cnt = extract_last_number_or_zero(cnt_items.getText())\n",
    "        else:\n",
    "            cnt = 0\n",
    "    else:\n",
    "        cnt = 0\n",
    "    return cnt\n",
    "\n",
    "def get_number_of_results_from_url(url):\n",
    "    url = url \n",
    "    t = time.time()\n",
    "    \n",
    "    cnt_shit = 1\n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "            if len(r.url.split(';'))!=len(url.split(';')):\n",
    "                print('not found: '+url)\n",
    "                return 0\n",
    "            soup = BeautifulSoup(r.text)\n",
    "            break\n",
    "        except Exception as e:  # This is the correct syntax\n",
    "            print ('some shit happend')\n",
    "            sleep(60*cnt_shit)\n",
    "            cnt_shit +=1\n",
    "\n",
    "    t = time.time() - t\n",
    "    print('crawled cnt pages; time [{0:.2f}s]'.format(t))\n",
    "    cnt_res = cnt_results_crawl(soup)\n",
    "    print('found: '+str(cnt_res)+' results at: '+url)\n",
    "    return cnt_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping loop - all pages based on URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_filename(s):\n",
    "#     value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore')\n",
    "#     value = unicode(re.sub('[^\\w\\s-]', '', value).strip().lower())\n",
    "#     value = unicode(re.sub('[-\\s]+', '-', value))\n",
    "#     return value\n",
    "    valid_chars = \"-_.() %s%s\" % (string.ascii_letters, string.digits)\n",
    "    filename = ''.join(c for c in s if c in valid_chars)\n",
    "    filename = filename.replace(' ','_') # I don't like spaces in filenames.\n",
    "    return filename\n",
    "\n",
    "\n",
    "def create_filename(url):\n",
    "    url = url.replace('https://knihy.heureka.cz/f:', '')\n",
    "    list_url = url.split(';')\n",
    "    final_list = []\n",
    "    for part in list_url:\n",
    "        part = part.replace(':', ',')\n",
    "        part_list = part.split(',')\n",
    "        part_list.sort()\n",
    "        final_list += part_list\n",
    "\n",
    "\n",
    "    final_list = list(set(final_list))\n",
    "    final_list.sort()\n",
    "    for i in final_list:\n",
    "        i = format_filename(i)\n",
    "        \n",
    "    search = '' + '-'.join(final_list) + '.csv'\n",
    "    return search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_all_pages_from_url_save_to_cvs(url, from_page = 1, max_pages = 10, timeout = 0.5, filename = None):\n",
    "        \n",
    "    if filename is not None:\n",
    "        print('using custom filename')\n",
    "    else:\n",
    "        filename = create_filename(url)\n",
    "        \n",
    "    cnt_scrapped = 0\n",
    "    \n",
    "\n",
    "    existing_file = False\n",
    "    # check for existing scrap\n",
    "    filename = 'scrapped/' + filename\n",
    "    try:\n",
    "        fh = open(filename, 'r')\n",
    "        df_tmp = pd.read_csv(filename)\n",
    "        cnt_scrapped = df.shape[0]\n",
    "        existing_file = True\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    t_total = time.time()\n",
    "    \n",
    "    # scrap number of results and pages\n",
    "    cnt_results = get_number_of_results_from_url(url)\n",
    "    cnt_pages = math.ceil(cnt_results/30.0)\n",
    "    \n",
    "    \n",
    "    # if max pages <= 0 ... then crawl all pages\n",
    "    to_page = cnt_pages\n",
    "    if max_pages < cnt_pages:\n",
    "        to_page = max_pages\n",
    "        \n",
    "    # allready fully scrapped\n",
    "    if cnt_scrapped == cnt_results:\n",
    "        print('allready fully scraped. nothing to do')\n",
    "        return\n",
    "    \n",
    "  \n",
    "    # partially scrapped\n",
    "    if cnt_scrapped > 0:\n",
    "        print('allready exisitng filename')\n",
    "        print('with [{}] results, from [{}] possible. [] remaining'.format(cnt_scrapped, cnt_results,  cnt_results - cnt_scrapped))\n",
    "        cnt_pages_scrapped = math.floor(cnt_scrapped/30.0)\n",
    "        from_page = cnt_pages_scrapped + 1\n",
    "        print('---------------------------------------------------------')\n",
    "        print('staring to scrap rest [{}] pages; limit [{}] = [{}] products'.format(cnt_pages - cnt_pages_scrapped, max_pages, max_pages*30))\n",
    "        print('---------------------------------------------------------')\n",
    "    \n",
    "    # not scrapped\n",
    "    else:\n",
    "        print('---------------------------------------------------------')\n",
    "        print('staring to scrap [{}] pages; limit [{}] = [{}] products'.format(cnt_pages, max_pages, max_pages*30))\n",
    "        print('---------------------------------------------------------')\n",
    "\n",
    "    res = pd.DataFrame(columns=['db_id', 'name', 'rating', 'reviews_cnt', 'year', 'pages_cnt', 'publisher', 'genre', 'author', 'lang', 'description', 'price_from', 'price_to', 'unparsed'])\n",
    "    \n",
    "    ###########################################################\n",
    "    # just for nicely padded print :-)\n",
    "    padding = 1\n",
    "    tmp = to_page\n",
    "    while tmp >= 10:\n",
    "        tmp /= 10\n",
    "        padding +=1\n",
    "    padd = 'scrapping [{0:0'+str(padding)+'d}'\n",
    "    ##########################################################\n",
    "    \n",
    "    # go through all pages\n",
    "    for i in range(from_page,to_page+1):        \n",
    "        # page\n",
    "        url_paged = url + '/?f=' + str(i) # # ...../?f=1\n",
    "        \n",
    "        print(padd.format(i), end = '')\n",
    "        print('/{}]'.format(to_page), end='')\n",
    "        # scrap it (+ measure time)\n",
    "        t1 = time.time()\n",
    "        \n",
    "        #################################################### fix all errors :-)\n",
    "        cnt_shit = 1\n",
    "        while True:\n",
    "            try:\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                break\n",
    "            except Exception as e:  # This is the correct syntax\n",
    "                print ('some shit happend')\n",
    "                sleep(60*cnt_shit)\n",
    "                cnt_shit +=1\n",
    "        ####################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        t1 = time.time() - t1\n",
    "        print('[{0:.2f}s]'.format(t1), end = '')\n",
    "        \n",
    "        # parse up to 30 product from page\n",
    "        products = soup.find_all('div', attrs={'class':'p'}) # ok safe now \n",
    "        for product in products:\n",
    "            parsed_product = parse_product(product)\n",
    "            res = res.append(pd.Series(parsed_product, index=res.columns), ignore_index=True) \n",
    "       \n",
    "        print('[OK] sleeping [{}s]'.format(timeout))\n",
    "        sleep(timeout)\n",
    "        \n",
    "        \n",
    "        # save often..\n",
    "        if i > 10 and  i % 10:\n",
    "            if existing_file:\n",
    "                with open(filename, 'a') as f:\n",
    "                    res.to_csv(f,  index=False, header=False)\n",
    "                    res = pd.DataFrame(columns=['db_id', 'name', 'rating', 'reviews_cnt', 'year', 'pages_cnt', 'publisher', 'genre', 'author', 'lang', 'description', 'price_from', 'price_to', 'unparsed'])\n",
    "            else:\n",
    "                res.to_csv(filename, index=False)\n",
    "                existing_file = True\n",
    "                res = pd.DataFrame(columns=['db_id', 'name', 'rating', 'reviews_cnt', 'year', 'pages_cnt', 'publisher', 'genre', 'author', 'lang', 'description', 'price_from', 'price_to', 'unparsed'])\n",
    "    \n",
    "    # final save\n",
    "    if existing_file:\n",
    "        with open(filename, 'a') as f:\n",
    "            res.to_csv(f,  index=False, header=False)\n",
    "    else:\n",
    "        res.to_csv(filename, index=False)\n",
    "\n",
    "    t_total = time.time() - t_total\n",
    "    print('---------------------------------------------------------')\n",
    "    print('scraped [{}] pages = [{}] products; '.format(to_page, len(res)), end = '')\n",
    "    print('total time [{0:.2f}s]'.format(t_total))\n",
    "    print('---------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_publisher_list(crawled_csv, min_cnt_occurences = 4, shuffle = True):\n",
    "    languages = ['anglické', 'chorvatské', 'dánské', 'francouzské', 'holandské', 'italské', 'japonské', 'latinské', 'maďarské', 'německé', 'polské', 'portugalské', 'ruské', 'slovenské', 'turecké', 'vietnamské', 'Česky', 'české', 'čínské', 'řecké', 'španělské']\n",
    "    genres = ['beletrie', 'cestopisy', 'dobrodružství, humor', 'duchovno', 'dětské', 'esoterika', 'historická beletrie', 'horory', 'kuchařky', 'křížovky', 'naučná', 'náboženství', 'osobní rozvoj', 'poezie', 'populárně naučná', 'pro ženy', 'Psychologický román', 'román', 'romány pro ženy', 'sci-fi a fantasy', 'slovníky', 'zdraví a životní styl', 'životopisy']\n",
    "    df = pd.read_csv(crawled_csv)\n",
    "    # misto jazyku None\n",
    "    df.loc[df[\"publisher\"].isin(languages), \"publisher\"] = None\n",
    "    # misto genre None\n",
    "    df.loc[df[\"publisher\"].isin(genres), \"publisher\"] = None\n",
    "    # dropni vsechny None\n",
    "    df = df.dropna(subset=['publisher'])\n",
    "    # count and sort\n",
    "    df = df.groupby('publisher').publisher.count().reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "    # only with higher num of occurences\n",
    "    r = df.loc[df['count'] >= min_cnt_occurences]\n",
    "    # create python list\n",
    "    list_of_publishers = r.publisher.tolist()\n",
    "    # replace some shit\n",
    "    for i in range(len(list_of_publishers)):\n",
    "        list_of_publishers[i] = list_of_publishers[i].lower() # to lower\n",
    "        list_of_publishers[i] = list_of_publishers[i].replace(\" \", \"%20\") # encode spaces\n",
    "        list_of_publishers[i] = list_of_publishers[i].replace(\"#\", \"\") # some shitty names\n",
    "        list_of_publishers[i] = list_of_publishers[i].replace(\"&amp;\", \"&\")\n",
    "        list_of_publishers[i] = list_of_publishers[i].replace(\"/\", \"%7C\")\n",
    "        list_of_publishers[i] = list_of_publishers[i].replace('á', 'a') \\\n",
    "        .replace('č', 'c') \\\n",
    "        .replace('ď', 'd') \\\n",
    "        .replace('é', 'e') \\\n",
    "        .replace('ě', 'e') \\\n",
    "        .replace('í', 'i') \\\n",
    "        .replace('ň', 'n') \\\n",
    "        .replace('ó', 'o') \\\n",
    "        .replace('ř', 'r') \\\n",
    "        .replace('š', 's') \\\n",
    "        .replace('ť', 't') \\\n",
    "        .replace('ú', 'u') \\\n",
    "        .replace('ů', 'u') \\\n",
    "        .replace('ý', 'y') \\\n",
    "        .replace('ý', 'y') \\\n",
    "        .replace('ž', 'z') \\\n",
    "        .replace('ľ', 'l')\n",
    "        list_of_publishers[i] = '_' + list_of_publishers[i]  # and add underscore at the beggin\n",
    "    # eventually randum shuffle\n",
    "    if shuffle:\n",
    "        random.shuffle(list_of_publishers)\n",
    "        \n",
    "    return list_of_publishers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predefined url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://knihy.heureka.cz/f:'\n",
    "\n",
    "predefined = [ {'pref':'4625', 'suffix' : ['16494','16495','16496','16497','16498']} ,\n",
    "               {'pref': '4620',  'suffix': ['1900','1905','1907','1911','1920','1925','1928','1930','1931','1934','1940','1946','1947','1948','1949','1950','1951','1952','1953','1954','1955','1956','1957','1958','1959','1960','1961','1962','1963','1964','1965','1966','1967','1968','1969','1970','1971','1972','1973','1974','1975','1976','1977','1978','1979','1980','1981','1982','1983','1984','1985','1986','1987','1988','1989','1990','1991','1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2025']},\n",
    "               {'pref' : '4619', 'suffix' : ['22929248','531655','268123','431096','29313706','27688452','259848','259855','26095413','27890885','555182','259836','24319385','174508','26737347','27593856','27596982','27688656','531656','24636535','531658','26152179','259828']},\n",
    "               {'pref' : '4627' ,'suffix' : ['66288','66300','21366767','69330','21277184','102484','70032','102483','133027','102482','67392','84188','79763','170155','93','21364634','17989634','21366766','133028','67579','102485']}             \n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# >> TODO << - logic constructing URL's\n",
    "* heureka me zarizla\n",
    "    * po 49  crawlech (timeout 0.5s)\n",
    "    * po 278 crawlech (timeout 5s)\n",
    "* -> chce to vychytat ty timeouty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordHelper(dicts, idx_dict  = 0):\n",
    "    prefix = dicts[idx_dict]['pref']\n",
    "    suffixes = dicts[idx_dict]['suffix']\n",
    "    return suffixes, prefix\n",
    "\n",
    "def logicHelper(predef, dicts, index, url, maxResults): # recursive adding indexes up to all used\n",
    "    if index!=0: # next query\n",
    "        url=url+';'\n",
    "        \n",
    "    # if we're out of predefined, use dict\n",
    "    if index==len(predef):\n",
    "        \n",
    "        # generate dict\n",
    "        suffixes, prefix = wordHelper(dicts, 0)\n",
    "        \n",
    "        # go through dict\n",
    "        for suffix in suffixes:\n",
    "            # and download whever possible\n",
    "            if get_number_of_results_from_url(url+ prefix +':'+suffix)<=maxResults:\n",
    "                scrap_all_pages_from_url_save_to_cvs(url+ prefix +':'+suffix)\n",
    "                \n",
    "            \n",
    "        # finally return. we have no other options ...\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        suffixes=predef[index]['suffix'] # suffixes only from current index\n",
    "        # random.shuffle(suffixes)\n",
    "        prefix = predef[index]['pref']\n",
    "    \n",
    "    while len(suffixes)>0: # till we have some suffiexes\n",
    "        lo=0\n",
    "        hi=len(suffixes)-1\n",
    "        mid=0\n",
    "        \n",
    "        while lo<hi:\n",
    "            mid=(int)((lo+hi+1)/2) \n",
    "            if get_number_of_results_from_url(url + prefix + ':' + ','.join(suffixes[:mid]))>maxResults:\n",
    "                hi=mid-1\n",
    "            else: # enough\n",
    "                lo=mid\n",
    "            sleep(0.5)\n",
    "            \n",
    "        mid = lo\n",
    "        # if too much results\n",
    "        if mid==0 and len(suffixes)>0:\n",
    "            if len(suffixes)==1 and get_number_of_results_from_url(url+ prefix +':'+suffixes[0])<=maxResults:\n",
    "                scrap_all_pages_from_url_save_to_cvs(url+ prefix +':'+suffixes[0])\n",
    "                return \n",
    "            \n",
    "            print( str(mid)+ \"   :   \"+str(len(suffixes)))\n",
    "            logicHelper(predef, dicts, index+1,url+ prefix +':'+suffixes[0],maxResults) # only first suffix \n",
    "            suffixes=suffixes[1:] # and remove it\n",
    "        \n",
    "        else: # enough\n",
    "            scrap_all_pages_from_url_save_to_cvs(url+ prefix +':'+','.join(suffixes[:mid]))\n",
    "            suffixes=suffixes[mid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawled cnt pages; time [3.81s]\n",
      "found: 153047 results at: https://knihy.heureka.cz/f:4625:16494,16497\n",
      "crawled cnt pages; time [3.03s]\n",
      "found: 98974 results at: https://knihy.heureka.cz/f:4625:16494\n",
      "0   :   5\n",
      "crawled cnt pages; time [11.49s]\n",
      "found: 57185 results at: https://knihy.heureka.cz/f:4625:16494;4620:2015,1952,1947,1967,1969,1954,1951,1995,1972,1911,2020,1978,2003,2001,2017,1989,1979,1960,2025,2005,1983,1925,2018,1949,1962,2012,1953,1992,1991,2004,1999,2010,2014,1948,1985,1982,1968,1993,2016,1996,1994,1990,1957\n",
      "crawled cnt pages; time [3.77s]\n",
      "found: 18478 results at: https://knihy.heureka.cz/f:4625:16494;4620:2015,1952,1947,1967,1969,1954,1951,1995,1972,1911,2020,1978,2003,2001,2017,1989,1979,1960,2025,2005,1983\n",
      "crawled cnt pages; time [4.17s]\n",
      "found: 6515 results at: https://knihy.heureka.cz/f:4625:16494;4620:2015,1952,1947,1967,1969,1954,1951,1995,1972,1911\n",
      "crawled cnt pages; time [5.05s]\n",
      "found: 6254 results at: https://knihy.heureka.cz/f:4625:16494;4620:2015,1952,1947,1967,1969\n",
      "crawled cnt pages; time [4.78s]\n",
      "found: 6242 results at: https://knihy.heureka.cz/f:4625:16494;4620:2015,1952\n",
      "crawled cnt pages; time [2.41s]\n",
      "found: 6242 results at: https://knihy.heureka.cz/f:4625:16494;4620:2015\n",
      "0   :   87\n",
      "crawled cnt pages; time [1.90s]\n",
      "found: 2038 results at: https://knihy.heureka.cz/f:4625:16494;4620:2015;4619:174508,24636535,259828,24319385,26152179,555182,27688452,26095413,27593856,259855,431096\n",
      "crawled cnt pages; time [3.75s]\n",
      "found: 253 results at: https://knihy.heureka.cz/f:4625:16494;4620:2015;4619:174508,24636535,259828,24319385,26152179\n",
      "crawled cnt pages; time [2.55s]\n",
      "found: 311 results at: https://knihy.heureka.cz/f:4625:16494;4620:2015;4619:174508,24636535,259828,24319385,26152179,555182,27688452,26095413\n",
      "crawled cnt pages; time [2.27s]\n",
      "found: 311 results at: https://knihy.heureka.cz/f:4625:16494;4620:2015;4619:174508,24636535,259828,24319385,26152179,555182\n",
      "crawled cnt pages; time [2.87s]\n",
      "found: 253 results at: https://knihy.heureka.cz/f:4625:16494;4620:2015;4619:174508,24636535,259828,24319385,26152179\n",
      "---------------------------------------------------------\n",
      "staring to scrap [9] pages; limit [10] = [300] products\n",
      "---------------------------------------------------------\n",
      "scrapping [1/9][2.57s][OK] sleeping [0.5s]\n",
      "scrapping [2/9][2.47s][OK] sleeping [0.5s]\n",
      "scrapping [3/9][2.64s][OK] sleeping [0.5s]\n",
      "scrapping [4/9][2.21s][OK] sleeping [0.5s]\n",
      "scrapping [5/9]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3b7dce4e559c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlogicHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredefined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdicts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-a03ac0c053f3>\u001b[0m in \u001b[0;36mlogicHelper\u001b[0;34m(predef, dicts, index, url, maxResults)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\"   :   \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mlogicHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxResults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# only first suffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# and remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a03ac0c053f3>\u001b[0m in \u001b[0;36mlogicHelper\u001b[0;34m(predef, dicts, index, url, maxResults)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\"   :   \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mlogicHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxResults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# only first suffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# and remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a03ac0c053f3>\u001b[0m in \u001b[0;36mlogicHelper\u001b[0;34m(predef, dicts, index, url, maxResults)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# enough\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mscrap_all_pages_from_url_save_to_cvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-b4ef672ae133>\u001b[0m in \u001b[0;36mscrap_all_pages_from_url_save_to_cvs\u001b[0;34m(url, from_page, max_pages, timeout, filename)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;31m# Resolve redirects if allowed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# Shuffle things around if there's history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;31m# Resolve redirects if allowed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# Shuffle things around if there's history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                     \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                     \u001b[0;34m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m                 )\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unexpected EOF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/OpenSSL/SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1813\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1814\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# generate some dicts\n",
    "publisher_list = generate_publisher_list('scrapped/test.csv', min_cnt_occurences = 5, shuffle = True)\n",
    "dicts = [    {'pref':'4621', 'suffix' : publisher_list}, # publisher\n",
    "             {'pref' : '4624' ,'suffix' : []},  # author\n",
    "             {'pref' : '19863' ,'suffix' : []}, # producer\n",
    "             {'pref' : 'q' ,'suffix' : []} # fulltext\n",
    "        ]\n",
    "\n",
    "\n",
    "logicHelper(predefined, dicts,0,base_url , 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO - GUI ???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
