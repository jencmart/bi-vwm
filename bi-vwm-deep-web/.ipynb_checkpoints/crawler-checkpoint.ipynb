{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import string\n",
    "import re\n",
    "from pprint import pprint\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import math\n",
    "from time import sleep\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some shitty parsing helpers funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_leading_tailing_spaces_commas(s):\n",
    "    s1 = re.sub(r\"^\\s+|\\s+$\", \"\", s)\n",
    "    s2 = re.sub(r\"^\\,+|\\,+$\", \"\", s1)\n",
    "    s3 = re.sub(r\"^\\s+|\\s+$\", \"\", s2)\n",
    "    return s3\n",
    "\n",
    "\n",
    "def extract_first_number_or_zero(dirty):\n",
    "    dirty = re.sub(r\"\\D+\", \" \", dirty)\n",
    "    lst = [int(s) for s in dirty.split() if s.isdigit()]\n",
    "    if len(lst) > 0:\n",
    "        return lst[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def extract_last_number_or_zero(dirty):\n",
    "    dirty = re.sub(r\"\\D+\", \" \", dirty)\n",
    "    lst = [int(s) for s in dirty.split() if s.isdigit()]\n",
    "    if len(lst) > 0:\n",
    "        return lst[-1]\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def parse_price(buy): #  SAFE\n",
    "    if buy is None:\n",
    "        return 0, 0, 0\n",
    "    # -- p -- class: price\n",
    "    price = buy.find('p', attrs={'class':'price'})\n",
    "    if price is None:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    item_price_from = price.find('span', attrs={'class':'priceFrom'})\n",
    "    if item_price_from is None:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    item_price_from = extract_first_number_or_zero(item_price_from.getText())\n",
    "    price_to = price.find('span', attrs={'class':'priceTo'})\n",
    "    if price_to is None:\n",
    "        item_price_to = item_price_from\n",
    "        item_shop_count = 1\n",
    "    else:\n",
    "        item_price_to = price_to.getText()\n",
    "        item_price_to = extract_first_number_or_zero(item_price_to)\n",
    "        # -- p -- shop count (only if multiple prices)\n",
    "        item_shop_count = buy.find('p', attrs={'class':'count'})\n",
    "        if item_shop_count is None:\n",
    "            return item_price_from, item_price_to, 1\n",
    "        item_shop_count = item_shop_count.find('a')\n",
    "        if item_shop_count is None:\n",
    "            return item_price_from, item_price_to, 1\n",
    "        item_shop_count = extract_first_number_or_zero(item_shop_count.getText())\n",
    "\n",
    "    return item_price_from, item_price_to, item_shop_count\n",
    "\n",
    "\n",
    "def parse_reviews(desc): # SAFE\n",
    "     # -- p -- class: rw\n",
    "    reviews = desc.find('p', attrs={'class':'rw'})\n",
    "    if reviews is None :\n",
    "        return 0, 0\n",
    "    \n",
    "    item_rating = reviews.find('span', attrs={'class':'rating'})\n",
    "    if item_rating is None:\n",
    "        return 0, 0\n",
    "    item_rating = item_rating.find('span', attrs={'class':'hidden'})\n",
    "    if item_rating is None:\n",
    "        return 0, 0\n",
    "    item_rating = extract_first_number_or_zero(item_rating.getText())\n",
    "    \n",
    "    item_reviews_cnt = reviews.find('span', attrs={'class':'review-count'})\n",
    "    if item_reviews_cnt is None :\n",
    "        item_reviews_cnt = 0\n",
    "    else:\n",
    "        item_reviews_cnt = item_reviews_cnt.find('a')\n",
    "        if item_reviews_cnt is None:\n",
    "            return 0, 0\n",
    "        item_reviews_cnt = item_reviews_cnt.getText() \n",
    "        item_reviews_cnt = extract_first_number_or_zero(item_reviews_cnt)                \n",
    "        \n",
    "    return item_rating, item_reviews_cnt\n",
    "        \n",
    "def parse_description(desc): # SAFE\n",
    "    # -- div -- class: desc\n",
    "    desc_text = desc.find('p', attrs={'class':'desc'})\n",
    "    if desc_text is None:\n",
    "        return ''\n",
    "    \n",
    "    item_description = ''\n",
    "    for itm in desc_text:\n",
    "        if  type(itm) is bs4.element.NavigableString:\n",
    "            item_description = itm.string\n",
    "    return item_description\n",
    "\n",
    "\n",
    "def parse_id_and_name(desc): # SAFE\n",
    "    # -- div -- class: product-container\n",
    "    item_container = desc.find('div', attrs={'class':'product-container'})\n",
    "    if item_container is None:\n",
    "        return '', ''\n",
    "    item_id = item_container.get('id')\n",
    "    item_name = item_container.find('h2').find('a').getText()\n",
    "    \n",
    "    return item_id, item_name\n",
    "  \n",
    "    \n",
    "languages = ['anglické', 'chorvatské', 'dánské', 'francouzské', 'holandské', 'italské', 'japonské', 'latinské', 'maďarské', 'německé', 'polské', 'portugalské', 'ruské', 'slovenské', 'turecké', 'vietnamské', 'Česky', 'české', 'čínské', 'řecké', 'španělské']\n",
    "genres = ['beletrie', 'cestopisy', 'dobrodružství, humor', 'duchovno', 'dětské', 'esoterika', 'historická beletrie', 'horory', 'kuchařky', 'křížovky', 'naučná', 'náboženství', 'osobní rozvoj', 'poezie', 'populárně naučná', 'pro ženy', 'Psychologický román', 'román', 'romány pro ženy', 'sci-fi a fantasy', 'slovníky', 'zdraví a životní styl', 'životopisy']\n",
    "# last always language\n",
    "# between numbers - author\n",
    "# genres -- matchneme\n",
    "# problem: no numbers -- co je author a co je nakaladatelstvi\n",
    "# ['Grada', 'dětské', 'Antonín Šplíchal, české']\n",
    "# dobrodružství, Panteon, 2019, Jonas Jonasson, 400, české\n",
    "\n",
    "def parse_hard_shit(s):\n",
    "    gen = ''\n",
    "    lng = ''\n",
    "    prod = ''\n",
    "    auth = ''\n",
    "    s1 = re.sub(r'<p class=\"params\">', \"\", s)\n",
    "    s1 = re.sub(r'<abbr title=\"Počet stran\">[0-9]+</abbr>,', 'XXX', s1) # pages\n",
    "    s1 = re.sub(r'<abbr title=\"Rok vydání\">[0-9]+</abbr>,', 'XXX', s1) # year\n",
    "    result = [x.strip() for x in s1.split(',')]\n",
    "\n",
    "    for l in languages:\n",
    "        if l+'</p>' == result[-1]:\n",
    "            lng = l\n",
    "            s1 = re.sub(l+'</p>', 'XXX', s1)\n",
    "            break\n",
    "\n",
    "    result = [x.strip() for x in s1.split(',')]\n",
    "    for g in genres:\n",
    "        for r in result:\n",
    "            if g == r:\n",
    "                gen = g\n",
    "                s1 = re.sub(g+',', 'XXX', s1)\n",
    "                break\n",
    "\n",
    "    # chybi - autor && nakladatelstvi\n",
    "\n",
    "    s1 = re.sub(r'XXX', '', s1) # year\n",
    "    s1 = remove_leading_tailing_spaces_commas(s1)\n",
    "    result = [x.strip() for x in s1.split(',')]\n",
    "    result\n",
    "\n",
    "    if len(result) == 1:\n",
    "        auth = result[0]\n",
    "\n",
    "    if len(result) == 2:\n",
    "        prod = result[0]\n",
    "        auth = result[1]\n",
    "\n",
    "    if len(result) > 2:\n",
    "        for r in reversed(result):\n",
    "            if (' ' in r):\n",
    "                auth = r\n",
    "                break\n",
    "          \n",
    "    if auth != '':\n",
    "        result.remove(auth)\n",
    "        if prod == '':\n",
    "            if len(result) > 0:\n",
    "                for r in result:\n",
    "                    if r.isalpha():\n",
    "                        prod = r\n",
    "                        break\n",
    "    \n",
    "    prod = remove_leading_tailing_spaces_commas(prod)\n",
    "    auth = remove_leading_tailing_spaces_commas(auth)\n",
    "    return prod, gen, auth, lng\n",
    "\n",
    "def parse_params(desc): # SAFE\n",
    "    # -- p -- class: params\n",
    "    publisher = ''\n",
    "    genre = ''\n",
    "    author = ''\n",
    "    lang = ''\n",
    "    year = 0\n",
    "    pages_cnt = 0\n",
    "    \n",
    "    params = desc.find('p', attrs={'class':'params'})\n",
    "    if params is None:\n",
    "        return year, pages_cnt, publisher, genre, author, lang, \"\"\n",
    "    \n",
    "    unknown = []\n",
    "    for param in params:\n",
    "        if  type(param) is bs4.element.NavigableString:\n",
    "            unknown.append(remove_leading_tailing_spaces_commas(param.string))\n",
    "        elif param.get('title') == 'Rok vydání':\n",
    "            year = extract_first_number_or_zero(param.getText())\n",
    "        elif param.get('title') == 'Počet stran':\n",
    "            pages_cnt = extract_first_number_or_zero(param.getText())\n",
    "    params = str(params)\n",
    "    publisher, genre, author, lang = parse_hard_shit(params)\n",
    "    return year, pages_cnt, publisher, genre, author, lang, unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID, name, rating, reviews_cnt, year, pages_cnt, publisher, genre, author, lang, description, price_from, price_to, unparsed\n",
    "def parse_product(product):\n",
    "   \n",
    "    # [DESCRIPTION DIV]\n",
    "    desc = product.find('div', attrs={'class':'desc'})  # ok always\n",
    "    ID, name = parse_id_and_name(desc)  # SAFE\n",
    "    rating, reviews_cnt = parse_reviews(desc)  # SAFE\n",
    "    year, pages_cnt, publisher, genre, author, lang, unparsed = parse_params(desc)  # SAFE\n",
    "    description = parse_description(desc)  # SAFE\n",
    "    \n",
    "    # [BUY DIV]\n",
    "    buy = product.find('div', attrs={'class':'buy'})\n",
    "    price_from, price_to, shop_count = parse_price(buy)  # SAFE\n",
    "    \n",
    "    return [ID, name, rating, reviews_cnt, year, pages_cnt, publisher, genre, author, lang, description, price_from, price_to, unparsed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main crawl-loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count results helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnt_results_crawl(soup):  # SAFE\n",
    "    cnt_items = soup.find('div',  attrs={'class':'sort-bottom'})\n",
    "   \n",
    "    if cnt_items != None:\n",
    "        cnt_items = cnt_items.find('div',  attrs={'class':'count'})\n",
    "        if cnt_items != None:\n",
    "            cnt = extract_last_number_or_zero(cnt_items.getText())\n",
    "        else:\n",
    "            cnt = 0\n",
    "    else:\n",
    "        cnt = 0\n",
    "    return cnt\n",
    "\n",
    "def get_number_of_results_from_url(url):\n",
    "    url = url \n",
    "    t = time.time()\n",
    "    \n",
    "    cnt_shit = 1\n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "            if len(r.url.split(';'))!=len(url.split(';')):\n",
    "                print('not found: '+url)\n",
    "                return 0\n",
    "            soup = BeautifulSoup(r.text)\n",
    "            break\n",
    "        except Exception as e:  # This is the correct syntax\n",
    "            print ('some shit happend')\n",
    "            sleep(60*cnt_shit)\n",
    "            cnt_shit +=1\n",
    "\n",
    "    t = time.time() - t\n",
    "    print('crawled cnt pages; time [{0:.2f}s]'.format(t))\n",
    "    cnt_res = cnt_results_crawl(soup)\n",
    "    print('found: '+str(cnt_res)+' results at: '+url)\n",
    "    return cnt_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping loop - all pages based on URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_filename(s):\n",
    "#     value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore')\n",
    "#     value = unicode(re.sub('[^\\w\\s-]', '', value).strip().lower())\n",
    "#     value = unicode(re.sub('[-\\s]+', '-', value))\n",
    "#     return value\n",
    "    valid_chars = \"-_.() %s%s\" % (string.ascii_letters, string.digits)\n",
    "    filename = ''.join(c for c in s if c in valid_chars)\n",
    "    filename = filename.replace(' ','_') # I don't like spaces in filenames.\n",
    "    return filename\n",
    "\n",
    "\n",
    "def create_filename(url):\n",
    "    url = url.replace('https://knihy.heureka.cz/f:', '')\n",
    "    list_url = url.split(';')\n",
    "    final_list = []\n",
    "    for part in list_url:\n",
    "        part = part.replace(':', ',')\n",
    "        part_list = part.split(',')\n",
    "        part_list.sort()\n",
    "        final_list += part_list\n",
    "\n",
    "\n",
    "    final_list = list(set(final_list))\n",
    "    final_list.sort()\n",
    "    for i in final_list:\n",
    "        i = format_filename(i)\n",
    "        \n",
    "    search = '' + '-'.join(final_list) + '.csv'\n",
    "    return search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_all_pages_from_url_save_to_cvs(url, filename = None, BenchMark=False,from_page = 1, max_pages = 10, timeout = 0.5):\n",
    "        \n",
    "    if filename is not None:\n",
    "        print('using custom filename')\n",
    "    else:\n",
    "        filename = create_filename(url)\n",
    "        \n",
    "    cnt_scrapped = 0\n",
    "    \n",
    "\n",
    "    existing_file = False\n",
    "    if not BenchMark:\n",
    "        \n",
    "        # check for existing scrap\n",
    "        filename = 'scrapped/' + filename\n",
    "        try:\n",
    "            fh = open(filename, 'r')\n",
    "            df_tmp = pd.read_csv(filename)\n",
    "            cnt_scrapped = df_tmp.shape[0]\n",
    "            existing_file = True\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    \n",
    "    t_total = time.time()\n",
    "    \n",
    "    # scrap number of results and pages\n",
    "    cnt_results = get_number_of_results_from_url(url)\n",
    "    cnt_pages = math.ceil(cnt_results/30.0)\n",
    "    \n",
    "    \n",
    "    # if max pages <= 0 ... then crawl all pages\n",
    "    to_page = cnt_pages\n",
    "    if max_pages < cnt_pages:\n",
    "        to_page = max_pages\n",
    "        \n",
    "    # force full scrape if BenchMark\n",
    "    if BenchMark:\n",
    "        cnt_scrapped=0\n",
    "        \n",
    "    # allready fully scrapped\n",
    "    if cnt_scrapped == cnt_results:\n",
    "        print('allready fully scraped. nothing to do')\n",
    "        return 0\n",
    "    \n",
    "  \n",
    "    # partially scrapped\n",
    "    if cnt_scrapped > 0:\n",
    "        print('allready exisitng filename')\n",
    "        print('with [{}] results, from [{}] possible. [] remaining'.format(cnt_scrapped, cnt_results,  cnt_results - cnt_scrapped))\n",
    "        cnt_pages_scrapped = math.floor(cnt_scrapped/30.0)\n",
    "        from_page = cnt_pages_scrapped + 1\n",
    "        print('---------------------------------------------------------')\n",
    "        print('staring to scrap rest [{}] pages; limit [{}] = [{}] products'.format(cnt_pages - cnt_pages_scrapped, max_pages, max_pages*30))\n",
    "        print('---------------------------------------------------------')\n",
    "    \n",
    "    # not scrapped\n",
    "    else:\n",
    "        print('---------------------------------------------------------')\n",
    "        print('staring to scrap [{}] pages; limit [{}] = [{}] products'.format(cnt_pages, max_pages, max_pages*30))\n",
    "        print('---------------------------------------------------------')\n",
    "\n",
    "    res = pd.DataFrame(columns=['db_id', 'name', 'rating', 'reviews_cnt', 'year', 'pages_cnt', 'publisher', 'genre', 'author', 'lang', 'description', 'price_from', 'price_to', 'unparsed'])\n",
    "    \n",
    "    ###########################################################\n",
    "    # just for nicely padded print :-)\n",
    "    padding = 1\n",
    "    tmp = to_page\n",
    "    while tmp >= 10:\n",
    "        tmp /= 10\n",
    "        padding +=1\n",
    "    padd = 'scrapping [{0:0'+str(padding)+'d}'\n",
    "    ##########################################################\n",
    "    \n",
    "    # go through all pages\n",
    "    for i in range(from_page,to_page+1):        \n",
    "        # page\n",
    "        url_paged = url + '/?f=' + str(i) # # ...../?f=1\n",
    "        \n",
    "        print(padd.format(i), end = '')\n",
    "        print('/{}]'.format(to_page), end='')\n",
    "        # scrap it (+ measure time)\n",
    "        t1 = time.time()\n",
    "        \n",
    "        #################################################### fix all errors :-)\n",
    "        cnt_shit = 1\n",
    "        while True:\n",
    "            try:\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                break\n",
    "            except Exception as e:  # This is the correct syntax\n",
    "                print ('some shit happend')\n",
    "                sleep(60*cnt_shit)\n",
    "                cnt_shit +=1\n",
    "        ####################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        t1 = time.time() - t1\n",
    "        print('[{0:.2f}s]'.format(t1), end = '')\n",
    "        \n",
    "        # parse up to 30 product from page\n",
    "        products = soup.find_all('div', attrs={'class':'p'}) # ok safe now \n",
    "        for product in products:\n",
    "            parsed_product = parse_product(product)\n",
    "            res = res.append(pd.Series(parsed_product, index=res.columns), ignore_index=True) \n",
    "       \n",
    "        print('[OK] sleeping [{}s]'.format(timeout))\n",
    "        sleep(timeout)\n",
    "        \n",
    "        \n",
    "        # save often..\n",
    "        if i > 10 and  i % 10:\n",
    "            if existing_file:\n",
    "                with open(filename, 'a') as f:\n",
    "                    res.to_csv(f,  index=False, header=False)\n",
    "                    res = pd.DataFrame(columns=['db_id', 'name', 'rating', 'reviews_cnt', 'year', 'pages_cnt', 'publisher', 'genre', 'author', 'lang', 'description', 'price_from', 'price_to', 'unparsed'])\n",
    "            else:\n",
    "                res.to_csv(filename, index=False)\n",
    "                existing_file = True\n",
    "                res = pd.DataFrame(columns=['db_id', 'name', 'rating', 'reviews_cnt', 'year', 'pages_cnt', 'publisher', 'genre', 'author', 'lang', 'description', 'price_from', 'price_to', 'unparsed'])\n",
    "    \n",
    "    # final save\n",
    "    if existing_file:\n",
    "        with open(filename, 'a') as f:\n",
    "            res.to_csv(f,  index=False, header=False)\n",
    "    else:\n",
    "        res.to_csv(filename, index=False)\n",
    "\n",
    "    t_total = time.time() - t_total\n",
    "    print('---------------------------------------------------------')\n",
    "    print('scraped [{}] pages = [{}] products; '.format(to_page, len(res)), end = '')\n",
    "    print('total time [{0:.2f}s]'.format(t_total))\n",
    "    print('---------------------------------------------------------')\n",
    "    print (cnt_results)\n",
    "    return cnt_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_publisher_list(crawled_csv, min_cnt_occurences = 4, shuffle = True):\n",
    "    languages = ['anglické', 'chorvatské', 'dánské', 'francouzské', 'holandské', 'italské', 'japonské', 'latinské', 'maďarské', 'německé', 'polské', 'portugalské', 'ruské', 'slovenské', 'turecké', 'vietnamské', 'Česky', 'české', 'čínské', 'řecké', 'španělské']\n",
    "    genres = ['beletrie', 'cestopisy', 'dobrodružství, humor', 'duchovno', 'dětské', 'esoterika', 'historická beletrie', 'horory', 'kuchařky', 'křížovky', 'naučná', 'náboženství', 'osobní rozvoj', 'poezie', 'populárně naučná', 'pro ženy', 'Psychologický román', 'román', 'romány pro ženy', 'sci-fi a fantasy', 'slovníky', 'zdraví a životní styl', 'životopisy']\n",
    "    df = pd.read_csv(crawled_csv)\n",
    "    # misto jazyku None\n",
    "    df.loc[df[\"publisher\"].isin(languages), \"publisher\"] = None\n",
    "    # misto genre None\n",
    "    df.loc[df[\"publisher\"].isin(genres), \"publisher\"] = None\n",
    "    # dropni vsechny None\n",
    "    df = df.dropna(subset=['publisher'])\n",
    "    # count and sort\n",
    "    df = df.groupby('publisher').publisher.count().reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "    # only with higher num of occurences\n",
    "    r = df.loc[df['count'] >= min_cnt_occurences]\n",
    "    # create python list\n",
    "    list_of_publishers = r.publisher.tolist()\n",
    "    # replace some shit\n",
    "    for i in range(len(list_of_publishers)):\n",
    "        list_of_publishers[i] = list_of_publishers[i].lower() # to lower\n",
    "        list_of_publishers[i] = list_of_publishers[i].replace(\" \", \"%20\") # encode spaces\n",
    "        list_of_publishers[i] = list_of_publishers[i].replace(\"#\", \"\") # some shitty names\n",
    "        list_of_publishers[i] = list_of_publishers[i].replace(\"&amp;\", \"&\")\n",
    "        list_of_publishers[i] = list_of_publishers[i].replace(\"/\", \"%7C\")\n",
    "        list_of_publishers[i] = list_of_publishers[i].replace('á', 'a') \\\n",
    "        .replace('č', 'c') \\\n",
    "        .replace('ď', 'd') \\\n",
    "        .replace('é', 'e') \\\n",
    "        .replace('ě', 'e') \\\n",
    "        .replace('í', 'i') \\\n",
    "        .replace('ň', 'n') \\\n",
    "        .replace('ó', 'o') \\\n",
    "        .replace('ř', 'r') \\\n",
    "        .replace('š', 's') \\\n",
    "        .replace('ť', 't') \\\n",
    "        .replace('ú', 'u') \\\n",
    "        .replace('ů', 'u') \\\n",
    "        .replace('ý', 'y') \\\n",
    "        .replace('ý', 'y') \\\n",
    "        .replace('ž', 'z') \\\n",
    "        .replace('ľ', 'l')\n",
    "        list_of_publishers[i] = '_' + list_of_publishers[i]  # and add underscore at the beggin\n",
    "    # eventually randum shuffle\n",
    "    if shuffle:\n",
    "        random.shuffle(list_of_publishers)\n",
    "        \n",
    "    return list_of_publishers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predefined url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://knihy.heureka.cz/f:'\n",
    "\n",
    "predefined = [ {'pref':'4625', 'suffix' : ['16494','16495','16496','16497','16498']} ,\n",
    "               {'pref': '4620',  'suffix': ['1900','1905','1907','1911','1920','1925','1928','1930','1931','1934','1940','1946','1947','1948','1949','1950','1951','1952','1953','1954','1955','1956','1957','1958','1959','1960','1961','1962','1963','1964','1965','1966','1967','1968','1969','1970','1971','1972','1973','1974','1975','1976','1977','1978','1979','1980','1981','1982','1983','1984','1985','1986','1987','1988','1989','1990','1991','1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2025']},\n",
    "               {'pref' : '4619', 'suffix' : ['22929248','531655','268123','431096','29313706','27688452','259848','259855','26095413','27890885','555182','259836','24319385','174508','26737347','27593856','27596982','27688656','531656','24636535','531658','26152179','259828']},\n",
    "               {'pref' : '4627' ,'suffix' : ['66288','66300','21366767','69330','21277184','102484','70032','102483','133027','102482','67392','84188','79763','170155','93','21364634','17989634','21366766','133028','67579','102485']}             \n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logic constructing URL's\n",
    "* heureka me zarizla\n",
    "    * po 49  crawlech (timeout 0.5s)\n",
    "    * po 278 crawlech (timeout 5s)\n",
    "* -> chce to vychytat ty timeouty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordHelper(dicts, idx_dict  = 0):\n",
    "        prefix = dicts[idx_dict]['pref']\n",
    "        suffixes = dicts[idx_dict]['suffix']\n",
    "        return suffixes, prefix\n",
    "\n",
    "def logicHelper(predef, dicts, index, url, maxResults, fileName=None, maxBenchMarkResults=100000000000, benchMark=False): # recursive adding indexes up to all used\n",
    "        if index!=0: # next query\n",
    "            url=url+';'\n",
    "        res_count=0   \n",
    "        # if we're out of predefined, use dict\n",
    "        if index==len(predef):\n",
    "\n",
    "            # generate dict\n",
    "            suffixes, prefix = wordHelper(dicts, 0)\n",
    "\n",
    "            # go through dict\n",
    "            for suffix in suffixes:\n",
    "                # and download whever possible\n",
    "                if get_number_of_results_from_url(url+ prefix +':'+suffix)<=maxResults:\n",
    "                    res_count+=scrap_all_pages_from_url_save_to_cvs(url+ prefix +':'+suffix,fileName,benchMark)\n",
    "\n",
    "\n",
    "            # finally return. we have no other options ...\n",
    "            return res_count\n",
    "\n",
    "        else:\n",
    "            suffixes=predef[index]['suffix'] # suffixes only from current index\n",
    "            # random.shuffle(suffixes)\n",
    "            prefix = predef[index]['pref']\n",
    "\n",
    "        while len(suffixes)>0 and res_count<maxBenchMarkResults: # till we have some suffiexes\n",
    "            lo=0\n",
    "            hi=len(suffixes)-1\n",
    "            mid=0\n",
    "\n",
    "            strsuff=\"\"\n",
    "            while lo<hi:\n",
    "                mid=(int)((lo+hi+1)/2) \n",
    "                if prefix=='4620':\n",
    "                    strsuff=suffixes[0]+\"-\"+suffixes[mid]\n",
    "                else:\n",
    "                    strsuff=','.join(suffixes[:mid+1])\n",
    "                if get_number_of_results_from_url(url + prefix + ':' + strsuff )>maxResults:\n",
    "                    hi=mid-1\n",
    "                else: # enough\n",
    "                    lo=mid\n",
    "                sleep(0.5)\n",
    "\n",
    "            mid = lo\n",
    "            if prefix=='4620':\n",
    "                strsuff=suffixes[0]+\"-\"+suffixes[mid]\n",
    "            else:\n",
    "                strsuff=','.join(suffixes[:mid+1])\n",
    "\n",
    "            # if too much results\n",
    "            if mid==0 and len(suffixes)>0:\n",
    "                if len(suffixes)==1 and get_number_of_results_from_url(url+ prefix +':'+strsuff)<=maxResults:\n",
    "                    res_count+=scrap_all_pages_from_url_save_to_cvs(url+ prefix +':'+strsuff,fileName,benchMark)\n",
    "                    return res_count\n",
    "\n",
    "                print( str(mid)+ \"   :   \"+str(len(suffixes)))\n",
    "                res_count+=logicHelper(predef, dicts, index+1,url+ prefix +':'+strsuff,maxResults,fileName,maxBenchMarkResults-res_count,benchMark) # only first suffix \n",
    "                suffixes=suffixes[1:] # and remove it\n",
    "\n",
    "            else: # enough\n",
    "                res_count+=scrap_all_pages_from_url_save_to_cvs(url+ prefix +':'+strsuff,fileName,benchMark)\n",
    "                suffixes=suffixes[mid:]\n",
    "        return res_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(size):\n",
    "    publisher_list = generate_publisher_list('scrapped/test.csv', min_cnt_occurences = 5, shuffle = True)\n",
    "    dicts = [    {'pref':'4621', 'suffix' : publisher_list}, # publisher\n",
    "                 {'pref' : '4624' ,'suffix' : []},  # author\n",
    "                 {'pref' : '19863' ,'suffix' : []}, # producer\n",
    "                 {'pref' : 'q' ,'suffix' : []} # fulltext\n",
    "            ]\n",
    "    t=time.time()\n",
    "    fileBenchMark=\"BenchMark\"\n",
    "    logicHelper(predefined, dicts, 0 ,base_url,300,fileBenchMark,size,True)\n",
    "    t=time.time()-t\n",
    "    print(\"time: \", t, \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scrape():\n",
    "    # generate some dicts\n",
    "    publisher_list = generate_publisher_list('scrapped/test.csv', min_cnt_occurences = 5, shuffle = True)\n",
    "    dicts = [    {'pref':'4621', 'suffix' : publisher_list}, # publisher\n",
    "                 {'pref' : '4624' ,'suffix' : []},  # author\n",
    "                 {'pref' : '19863' ,'suffix' : []}, # producer\n",
    "                 {'pref' : 'q' ,'suffix' : []} # fulltext\n",
    "            ]\n",
    "\n",
    "\n",
    "    logicHelper(predefined, dicts,0,base_url , 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run_benchmark(1000)\n",
    "# run_scrape()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
